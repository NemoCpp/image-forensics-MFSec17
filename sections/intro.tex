\section{INTRODUCTION}
%% TODO: introduce Content Analysis and role in DARPA MEMEX

Researchers on our team have borne witness over the past two years to the ease and availability of potentially criminal goods and services on the modern Internet. In particular, our team working on the DARPA MEMEX project has focused on the issues of online gun sales, as such sales can have dramatic consequences in that they provide a medium for buyers and sellers to obviate traditional background checks; to proliferate the sale of dangerous semi-automatic weapons, and these sales can lead directly to loss of human life. For instance, a New York Police Department (NYPD) investigation in 2013 identified guns used in one suicide and four murders and traced their origin to transactions on the website \url{armslist.com}\cite{raja_2016}. 

The ability to rapidly detect and analyze gun sales transactions is a significant challenge. There are hundreds of both national and regional gun sales sites like Armlist, for instance, \url{floridaguntrader.com}, or \url{gunbroker.com}. Besides the sheer number of sites, many of the sites share common themes indicative of today's {\em Deep} web. They either require a login to buy, or sell - making bulk analysis difficult for traditional web crawlers. A significant number of the sites use Ajax/Javascript for pagination, or for displaying gun images from an ad; also making automatic analysis a challenge. But most significantly, the actual content required to determine the answers to significant questions regarding these weapons (``is this an automatic weapon?'', ``is this a long or a short gun?'', ``are there multiple weapons being sold'') are the {\em images} of the weapons themselves. 

We have previously worked on bulk image analysis from the deep web as it relates to human trafficking data \cite{mattmann7tg} using the Apache Tika content detection and analysis framework \cite{mattmann2011tika}. Our work there focused on image metadata forensics as an alternative to image pixel based analyses and object detection and recognition. Though metadata forensics were promising in human trafficking, based on our study of over 80 websites and online forums that specialize in the exchange of weapons we found it necessary to apply pixel-based analyses and object recognition in order to automatically discern whether or not the guns being sold are automatic, or semi-automatic; whether they have been stolen; and whether the transactions are potentially illegal. Automatically being able to discern these types of object properties in bulk analyses of image data has the potential to thwart crimes, and ultimately to save lives.

Object recognition is a standard problem in computer vision which deals with the recognition of objects of interest in the graphical data. In the context of images it is often called as image recognition. Historically image recognition was a challenging task and its accuracy of the recognized objects were much lower than average Human performance. However, due to the recent advancements in deep neural networks and availability of larger datasets with faster computing resources, we now have systems which have nearer or better performance than average human beings\cite{karpathy-cnn-compare}.

Today's deep learning frameworks are focused towards performance gain from native code and GPU optimization for fast matrix manipulations. One of the most popular and well-documented deep-learning systems is Google's Tensorflow \cite{abadi2016tensorflow}. Tensorflow is a scalable, Python-based system and it natively supports image recognition via its {\em Inception} model \cite{abadi2016tensorflow}. Inception provides a neural network trained on the ImageNet corpus \cite{krizhevsky2012imagenet}, a dataset of 14,197,122 images and classified using the WordNet taxonomy. As such, Tensor 

 In our case, Tensorflow does not provide out of the box bindings to Java based frameworks. Apache Tika is primarily written in Java and thus integrating with Tensorflow is not straight forward like any other JVM compatible libraries. In this paper we explore various methods of integration and their pros and cons.


%The objectives of Content Analysis are interpreting its meaning and establishing relationships \cite{}. Content analysis spreads across multimedia types like text in plain as well as rich formats, audio, image and video. //TODO: more here

% The objectives of Content Analysis are interpreting its meaning and establishing relationships \cite{}. Content analysis spreads across multimedia types like text in plain as well as rich formats, audio, image and video. //TODO: more here

% Defense Advanced Research Projects Agency (DARPA) started a program called MEMEX\cite{fbo-memex} to help law keeping agencies to monitor and counteract the illegal activities on the web. The first challenge in such systems is to retrieve dark and deep content from the World Wide Web. Understanding the meaning and relationships are essential for performing various activities like filtering, classification, and ranking the severity etc. While humans can only efficiently and precisely analyze a small set of datasets, considering the scale of the dark and deep Web, automating the process is inherent requirement for its mission.

%The term content analysis is usually used in reference to automated reading and interpretation of text, which can come from a variety of sources including the web, books, speeches, videos, and images. This generally involves approaches where textual information is present in different types of multimedia (e.g. audio, image, video). However, recent progress in image-focused deep learning methods has narrowed the gap between content analysis and computer vision. Advances in object recognition now allow us to detect actions, objects, and context from visual mediums with unprecedented granularity and accuracy. This has expanded the domain of Content Analysis to textual information that is derived rather than explicit, and this latent information often includes important contextual details. 

%In terms of content, the web is extremely diverse and dynamic. This makes it extremely difficult to collect data and then understand and contextualize it. Commercial searches engines have had great success helping general users find relevant information, but the government requires deeper content analysis capabilities across a larger portion of the web. This lead the Defense Advanced Research Projects Agency (DARPA) to create a program called Memex \cite{fbo-memex} to help law enforcement agencies monitor content from the deep and dark web - content that isn't indexed by commercial search engines. (BLURB ON DEFINITION) Deep and dark web content is an attractive place for criminals to obfiscate illicit activity such as human trafficking and the illicit trade of weapons, and law enforcement agencies don't have the manpower to effectively monitor the breadth of web content possibly related to these activities. For these reasons, a large portion of the Memex program has focused on content analysis approaches that enable the automatic identification of suspicious activity present in web-based text, images, video, and audio.

%% TODO: Introduce Tika and content analysis
%One of the foundational content analysis frameworks used in Memex is Apache Tika\cite{mattmann2011tika}, a Free and Open Source (FOSS) system that can detect and extract metadata and information from raw content through the identification of MIME and file types. It also has auto-detection features that enable it to selectively apply a rich pool of parsers to extract content. However, reading or parsing content is a low-level task compared to the complexity of interpreting content semantics. In this paper we describe useful enrichments to the framework that integrate recent innovations in the Artificial Intelligence (AI) domain to extend automated content analysis. While the amount of rich content is increasing, a vast majority of the web consists of either plain or rich text \cite{mphillips-EOT2012}. Recently, Apache Tika added support for information extraction by the application of Natural Language Processing\cite{TikaAndNER}. The newer version of Tika can be configured to recognize names by delegating named-entity recognition (NER) tasks to popular Natural Language Processing toolkits like Stanford CoreNLP\cite{Finkel:2005:INI:1219840.1219885}, Apache OpenNLP\cite{ApacheOpenNLP}, and MIT Lincoln Lab's MITIE \cite{MITIE-github}. The parallel integration of NLP frameworks in and tools for analyzing graphical content has created unique opportunities to characterize diverse content and capture the vast majority of web-based content.

%% TODO: introduce Image net

%% TODO: introduce Tensorflow

%% TODO: Describe the bigger challenge & Summarize how all these fit together
%As of October 2016, the most popular deep learning frameworks are focused towards performance gain from native code and GPU optimization for fast matrix manipulations. In our case, Tensorflow does not provide out of the box bindings to Java based frameworks. Apache Tika is primarily written in Java and thus integrating with Tensorflow is not straight forward like any other JVM compatible libraries. In this paper we explore various methods of integration and their pros and cons.

%% brief overview of rest of the paper
The organization of the rest of this paper is as follows: section \ref{sec:memex} provides an overview of the analysis task in which we describe data collection, analysis. The section \ref{sec:integration} describes the integration techniques we tried and the section \ref{sec:evaluation} provides the statistics on results, cross validation, and the run times of integration techniques.
